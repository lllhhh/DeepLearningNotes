{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络是由多个神经元组成。一个神经元由以下几个关键知识点组成：\n",
    "- 激活函数；\n",
    "- 损失函数；\n",
    "- 梯度下降。\n",
    "# 神经元的拟合原理\n",
    "单个神经元的网络模型如图所示：\n",
    "![单个神经元](imgs/10_neure.png)\n",
    "计算公式为 $$ z=\\sum_{i=1}^{n}w_i \\times x_i+b=w \\cdot x+b  $$\n",
    "z为输出的结果；x为输入；W为权重；b为偏执。W和b可以理解为两个变量。\n",
    "\n",
    "模型每次的学习都是为了调整W和b从而得到一个合适的值，最终由这个值配合运算公式所形成的逻辑就是神经网络的模型。\n",
    "## 正向传播\n",
    "\n",
    "## 反向传播\n",
    "\n",
    "### BP算法简介\n",
    "BP算法又称“误差反向传播算法”。我们最终的目的，是要让**正向传播的输出结果与标签间的误差最小化**，这就是反向传播的核心思想。\n",
    "\n",
    "# 激活函数--加入非线性因素，解决线性模型缺陷\n",
    "激活函数的主要作用就是加入非线性因素，解决线性模型缺陷。\n",
    "\n",
    "因为神经网络的数学基础是处处可微的，所以选取的激活函数要能保证数据输入与输出也是可微的。\n",
    "\n",
    "神经网络里常用的激活函数有Sigmoid、Tanh和Relu等。\n",
    "## Sigmoid函数\n",
    "### 函数介绍\n",
    "Sigmoid函数是常用的非线性激活函数，数学形式：$$ f(x)=\\frac{1+e^{-x}}{1} $$,曲线如图：![Sigmoid](imgs/10_sigmoid.png)\n",
    "\n",
    "x可以是正无穷到负无穷，但是对应得y却只有0~1的范围，所以经过Sigmoid函数输出的函数都会落在0~1的区间里，即Sigmoid函数能把输入的值“压缩”到0~1之间。\n",
    "\n",
    "TensorFlow中对应的函数为：\n",
    "___\n",
    "```Python\n",
    "tf.nn.sigmoid(x,name=None)\n",
    "```\n",
    "___\n",
    "从图像上看，随着x趋近正负无穷大，y对应的值越来越接近1或者-1，这种情况叫做饱和。所以，为了能有效使用Sigmoid函数，其极限也只能是-6~6之间，而-3~3之间应该会有比较好的效果。\n",
    "\n",
    "## Tanh函数\n",
    "Tanh函数可以说是Sigmoid函数的值域升级版，由Sigmoid函数的0~1之间升级到-1~1。但是Tanh函数也不能完全替代Sigmoid函数，在某些输出需要大于0的情况下，还是要用Sigmoid函数。\n",
    "### 函数介绍\n",
    "Tanh函数也是常用的非线性激活函数，其数学形式：\n",
    "$$ tanh(x)=2sigmoid(2x)-1 $$\n",
    "其函数曲线如图，\n",
    "![Tanh函数](imgs/10_tanh.png)，其x取值也是从正无穷到负无穷，对应的y值变为-1~1之间，相对于Sigmoid函数有更广的值域。\n",
    "### TensorFlow中对应的函数\n",
    "___\n",
    "```Python\n",
    "tf.nn.tanh(x,name=None)\n",
    "```\n",
    "___\n",
    "> Tanh函数跟S为gmoid函数有一样的缺陷，也是饱和问题，所以在使用Tanh函数式，要注意输入值的绝对值不能过大，否则模型无法训练。\n",
    "\n",
    "## ReLu函数\n",
    "### 函数介绍\n",
    "除了Sigmoid、Tanh函数外，还有一个更为常用的激活函数（也称为Rectifier）。其数学形式是：\n",
    "$$ f(x)=max(0,x) $$，大于0的留下，否则一律为0.ReLu函数应用的广泛性与它的优势是分不开的。这种对**正向信号的重视**，忽略了负向信号的特性，与我们人类神经元细胞对信号的反映极其相似。由于ReLu函数运算简单，大大提升了机器的运行效率,也是ReLu函数一个很大的优点。\n",
    "![ReLu](imgs/10_ReLu.png) \n",
    "与ReLu函数类似的还有Softplus函数。二者区别在于：Softplus函数会更加平滑，但是计算量很大，而且对于小于0的值保留的相对更多一点。Softplus函数公式：\n",
    "$$ f(x)=ln(1+e^x) $$\n",
    "虽然ReLu函数在信号相应上有很多优势，但这仅仅在正向传播方面。由于其对负值的全部舍去，因此很容易使模型输出全0从而无法再进行训练。这显然不是想要的结果，于是在基于ReLu的基础上演化出了一些变种函数：\n",
    "- **Noisy relus**:为max中的x加了一个高斯分布的噪声：\n",
    "![noisy_relus](imgs/10_noisy_relus.png)\n",
    "- **Leaky relus**:在Relu基础上，保留一部分负值，让x为负时乘0.01，即Leaky relus对负信号不是一味地拒绝，而是缩小。\n",
    "$$ f(x)=\\begin{cases}\n",
    "x(x>0)\\\\\n",
    "0.01x(otherwise)\n",
    "\\end{cases} $$ \n",
    "- 再进一步让这个0.01作为参数可调，于是，当x小于0时，乘以a,a小于等于1.\n",
    "$$ f(x)=\\begin{cases}\n",
    "x(x>0)\\\\\n",
    "ax(otherwise) \n",
    "\\end{cases} \n",
    "=>f(x)=max(x,ax)$$ 得到Leaky relus公式max(x,ax)\n",
    "\n",
    "- Elus ：当x小于0时，做了更复杂的变换，\n",
    "$$ f(x)=\\begin{cases} \n",
    "x(x(\\geq0))\\\\\n",
    "a(e^x -1)(otherwise)\n",
    "\\end{cases} \n",
    "$$\n",
    "\n",
    "Elus函数激活函数与ReLU函数一样都是不带参数的，而且收敛速度比Relu函数更快，使用Elus函数时，不使用批处理比使用批处理能够获得更好的效果，同时Elus函数不使用批处理的效果比Relu函数加批处理的效果更好。\n",
    "## TensorFlow中对应的函数\n",
    "TensorFlow中，关于ReLU函数的实现，有两个对应的函数：\n",
    "- tf.nn.relu(features,name=None):是一般的ReLU函数，即max(feature,0);\n",
    "- tf.nn.relu6(features,name=None):是以6为阈值的Relu函数，即min(max(features,0),6)。\n",
    "\n",
    "> relu6存在的原因是**防止梯度爆炸**，当节点和层数特别多而且输出都为正时，它们的加和会是一个很大的值，尤其在经历几层变换后，最终的值可能会离目标值相差太远。误差太大，会导致对参数调整修正值过大，这会导致网络抖动得较厉害，最终很难收敛。\n",
    "\n",
    "在TensorFlow中，softplus函数对应的函数：\n",
    "```Python\n",
    "tf.nn.softplus(features,name=None)\n",
    "```\n",
    "Elus函数对应的函数：\n",
    "```Python\n",
    "tf.nn.elu(features,name=None)\n",
    "```\n",
    "Leaky relus公式没有专门的函数，不过可以利用现有的函数组成而得到：\n",
    "```\n",
    "tf.maxinum(x,leak*x,name=name) #leak为传入的参数，可以设为0.01等\n",
    "```\n",
    "\n",
    "### Swish函数\n",
    "Swish函数是谷歌公司发现的一个效果更优于Relu的激活函数。经测试，在保持所有模型参数不变的情况下，只是把原来模型中的ReLU激活函数修改为Swish函数，模型的准确率均有所提升。其公式：$$ f(x)=x\\times sigmoid(\\beta x) $$\n",
    "，其中$ \\beta $为x的缩放参数，一般情况默认1。TensorFlow中没有手动封装Swish函数：\n",
    "___\n",
    "```Python\n",
    "def Swish(x,beta=1):\n",
    "    return x * tf.nn.sigmoid(x*beta)\n",
    "```\n",
    "___\n",
    "### 激活函数总结\n",
    "- 神经网络中，运算特征是不断进行循环计算，所以在每代循环过程中，每个神经元的值也是在不断变化。这就导致Tanh函数在特征相差明显时的效果会很好，在循环过程中其会不断扩大特征效果并显示出来。\n",
    "- 有时当计算的特征间的相差虽比较复杂却没有明显区别，或是特征间的相差不是特别大时，就需要更细微的分类判断，这时Sigmoid函数的效果就会更好一点。\n",
    "- Relu激活函数的优势是，经过其处理后的数据有更好的稀疏性。即，将数据转化为只有最大数值，其他都是0.这种变换可以近似程度地最大保留数据特征，用大多数元素为0的稀疏矩阵来实现。\n",
    "- 实际上，神经网络在不断反复计算中，就变成了Relu函数在不断尝试如何用一个大多数为0的矩阵来表达数据特征。以稀疏性数据来表达原有数据特征的方法，使得神经网络在迭代运算中能够取得又快又好的效果，所以目前大多用max(0,x)来代替Sigmoid函数。\n",
    "\n",
    "# SoftMax算法--处理分类问题\n",
    "## 什么是softmax\n",
    "softmax,就是如果判断输入属于某一个类的概率大于属于其他类的概率，那么这个类对应的值就逼近于1，其他类的值就逼近于0。该算法的主要应用就是多分类，而且是互斥的，即只能属于其中的一个类。与Sigmoid类的激活函数不同的是，一般的激活函数只能分两类，多以可以理解为Softmax是Sigmoid函数的扩展，算法式：$$ softmax=exp(logits)/reduce\\underline{}sum(exp(logits),dim) $$，把所有值用e的n次方计算出来，求和后算每个值占的比率，保证总和为1，一般就可以认为softmax得出的就是概率。这里exp(logits)指的就是$ e^{logots} $。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
