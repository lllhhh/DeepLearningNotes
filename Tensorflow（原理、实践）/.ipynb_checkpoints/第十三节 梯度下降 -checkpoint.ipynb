{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降--让模型逼近最小偏差\n",
    "# 梯度下降的作用及分类\n",
    "梯度下降法是一个最优算法，常用于机器学习和人工智能中递归性地逼近最小偏差模型，梯度下降的方向也就是用负梯度方向为搜索方向，沿着梯度下降的方向求解极小值。\n",
    "\n",
    "在训练过程中，每次的正向传播后都会得到输出值与真实值的损失值，这个损失值越小，代表模型越好，于是梯度下降的算法就用在这里，帮助寻找最小的那个损失值，从而可以反推出对应的学习参数b和w，达到优化模型的效果。\n",
    "\n",
    "> 常用的梯度下降方法可以分为：**批量梯度下降**、**随机梯度下降**和**小批量梯度下降**。\n",
    "\n",
    "- 批量梯度下降：遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度和更新梯度。这种方法每更新一次参数，都要把数据集里的所有样本看一遍，计算量大，计算速度慢，不支持在线学习，成为Batch gradient descent，批量梯度下降。\n",
    "- 随机梯度下降：每看一个数据就算一下损失函数，然后求梯度更新参数，这称为stochastic gradient descent，随机梯度下降。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，命中不到最优点。两次参数的更新也有可能互相抵消，造成目标函数震荡比较剧烈。\n",
    "- 小批量梯度下降：为了克服上面两种方法的缺点，一般采用一种折中手段--小批量梯度下降。这种方法把数据分为若干个批次，按批来更新参数，这样一批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。\n",
    "\n",
    "# TensorFlow中的梯度下降函数\n",
    "在TensorFlow中通过一个叫Optimizer的优化器类进行训练优化的。对于不同算法的优化器，在TensorFlow中会有不同的类。\n",
    "![梯度下降优化器](imgs/13_gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练过程中，先实例一个优化函数如tf.train.GradientDescentOptimizer,并基于一定的学习率进行梯度优化训练：\n",
    "___\n",
    "```Python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "```\n",
    "___\n",
    "接着使用一个minimize()操作，里面传入损失值节点loss，再启动一个外层的循环，优化器就会按照循环的次数一次次沿着loss最小值的方向优化参数。\n",
    "\n",
    "整个过程中的求导和反向传播操作，都是在优化器里自动完成的。目前比较常用的优化器为**Adam**优化器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 退化学习率--在训练的速度和精度之间找到平衡\n",
    "设置学习率的大小，是在精度和速度之间找到一个平衡：\n",
    "- 如果学习率的值太大，则训练速度会提升，但结果精度不够；\n",
    "- 如果学习率的值比较小，精度虽然提升了，但训练会耗费太多时间；\n",
    "\n",
    "退化学习率又叫学习率衰减，它的本意就是希望在训练过程中对于学习率大和小的优点都能够为我们所用，当训练刚开始时使用大的学习率加速训练，训练到一定程度后使用小的学习率来提高精度，这时可以使用学习率衰减的方法：\n",
    "___\n",
    "```Python\n",
    "def exponential_decay(learning_rate,global_step,decay_steps,decay_rate,staircase=False,name=None):\n",
    "```\n",
    "___\n",
    "学习率的衰减速度是由global_step和decay_steps来决定的。\n",
    "___\n",
    "```Python\n",
    "decayed_learning_rate = learning_rate*decay_rate^(global_step/decay_steps)\n",
    "```\n",
    "___\n",
    "\n",
    "staircase值默认为False。当为True时，将没有衰减功能，只是使用上面的公式初始化一个学习率的值而已。\n",
    "___\n",
    "```Python\n",
    "learning_rate=tf.train.exponential_decay(starter_learning_rate,global_step,100000,0.96)\n",
    "```\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
