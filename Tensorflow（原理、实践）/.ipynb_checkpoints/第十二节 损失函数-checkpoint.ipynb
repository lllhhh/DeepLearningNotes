{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 损失函数--用真实值与预测值的距离来知道模型的收敛方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数介绍\n",
    "损失函数的作用是用于描述模型预测值与真实值的差距大小。一般有两种常见的算法--**均值平方差(MSE)**和**交叉熵** 。\n",
    "## 1. 均值平方差\n",
    "均值平方差（Mean Squared Error,MSE），也称“均方误差”，在神经网络中主要是表达预测值与真实值之间的差异，在数理统计中，均方误差是指参数估计值与参数真值之差平方的期望值。公式为：$$ MSE= \\dfrac{1}{n} \\sum_{t=1}^n (observde_t - predicted_t)^2 $$\n",
    "均方误差的值越小，表明模型越好。类似的损失算法还有均方根误差**RMSE(将MSE开平方)**、**平均绝对值误差MAD（对一个真实值与预测值相减的绝对值取平方值**）。\n",
    "> 在神经网络中，预测值与真实值控制在同样的数据分布内，假设将预测值经过Sigmoid激活函数得到取值范围在0~1之间，那么真实值也归一化成0~1之间。这样在做loss计算时才会有好的效果。\n",
    "\n",
    "## 2.交叉熵\n",
    "交叉熵（crossentropy）也是loss算法的一种，一般用在分类问题上，表达的意思为预测输入样本属于某一类的概率。表达式：\n",
    "$$ c=-\\dfrac{1}{n} \\sum_{x} [y lna + (1-y)ln(1-a)] $$\n",
    "其中y代表真实值分类(0或1)，a代表预测值。交叉熵也是值越小，代表预测结果越准确。\n",
    "> 这里用于计算的a也是通过分布统一化处理的（或者是经过Sigmoid函数激活的），取值范围在0~1之间。如果真实值和预测值都是1，前面一项y*ln(a)就是1*ln(1)等于0，后一项(1-y)*ln(1-a)也就是0*ln(0)等于0，loss为0，反之loss函数为其他函数。\n",
    "## 损失算法的选取\n",
    "损失函数的选取取决于输入标签数据的类型：如果输入的是实数、无界的值，损失函数使用平方差，如果输入标签是位矢量（分类标志），使用交叉熵会更合适。\n",
    "\n",
    "# TensorFlow中常见loss函数\n",
    "## 1.均值平方差\n",
    "在TensorFlow中没有单独的MSE函数，不过由于公式比较简单，往往开发者会自己组合：\n",
    "___\n",
    "```Python\n",
    "MSE = tf.reduce_mean(tf.pow(tf.sub(logits,outputts)))\n",
    "MSE = tf.reduce_mean(tf.sqar(tf.sub(logits,outputs)))\n",
    "MSE = tf.reduce_mean(tf.sqar(logits-outputs))\n",
    "```\n",
    "___\n",
    "代码中logits代表标签值，outputs代表预测值。也可以组合成其他函数：\n",
    "___\n",
    "```Python\n",
    "Rmse = tf.sqrt(tf.reduce_mean(tf.pow(tf.sub(logits,outputs),2.0)))\n",
    "mad = tf.reduce_mean(tf.complex_abs(tf.sub(logits,outputs)))\n",
    "```\n",
    "___\n",
    "## 2.交叉熵\n",
    "在TensorFlow中常见的交叉熵函数有:\n",
    "- Sigmoid交叉熵\n",
    "- softmax交叉熵\n",
    "- Sparse交叉熵\n",
    "- 加权Signoid交叉熵。\n",
    "在TensorFlow中常用的损失函数表：\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>操作</th>\n",
    "        <th>描述</th>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <th>tf.nn.sigmoid_cross_entropy_with_logits(logits,targets,name=None)</th>\n",
    "        <th>计算输入logits和targets的交叉熵</th>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <th>tf.nn.softmax_cross_entropy_with_logits(logits,labels,name=None)</th>\n",
    "        <th>计算logits和labels的softmax交叉熵 Logits和Label必须为相同的shape与数据类型</th>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <th>tf.nn.sparse_softmax_cross_entropy_with_logits(logits,labels,name=None)</th>\n",
    "        <th>计算logits和labels的softmax交叉熵,与softmax_cross_entropy_with_logits功能一样，区别在于sparse_softmax_cross_entropy_with_logits的样本真实值与预测结果不需要one_hot编码，但是要求分类的个数一定要从0开始。假如分2类，那么标签的预测值只有0和1两个数。如果是5类，就是01234这5个数</th>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <th>tf.nn.weighted_cross_entropy_with_logits(logits,targets,pos_weight,name=None)</th>\n",
    "        <th>在交叉熵的基础上给第一项乘以一个系数**（加权）**，是增加或减少正样本在计算交叉熵时的损失值</th>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "# softmax算法与损失函数的综合应用\n",
    "## 交叉熵实验\n",
    "### 实例描述\n",
    "下面代码，假设有一个标签labels和一个网络输出值logits。\n",
    "这个实例就是以这两个值来进行以下3次实验。\n",
    "1. 两次softmax实验：将输出值logits分别进行1次和2次softmax，观察两次的区别及意义。\n",
    "2. 观察交叉熵：将步骤1中的两个值分别进行softmax_cross_entropy_with_logits,观察它们的区别。\n",
    "3. 自建公式实验：将做两次softmax的值放到自建组合的公式里得到正确的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-971c989c10b3>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "scaled= [[0.01791432 0.00399722 0.97808844]\n",
      " [0.04980332 0.04506391 0.90513283]]\n",
      "scaled2= [[0.21747023 0.21446465 0.56806517]\n",
      " [0.2300214  0.22893383 0.5410447 ]]\n",
      "rel1= [0.02215516 3.0996735 ]\n",
      "rel2= [0.56551915 1.4743223 ]\n",
      "rel3= [0.02215518 3.0996735 ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "labels = [[0,0,1],[0,1,0]]\n",
    "logits = [[2,0.5,6],\n",
    "          [0.1,0,3]]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "logits_scaled = tf.nn.softmax(logits)\n",
    "logits_scaled2 = tf.nn.softmax(logits_scaled)\n",
    "\n",
    "result1 = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "result2 = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits_scaled)\n",
    "result3 = -tf.reduce_sum(labels*tf.log(logits_scaled),1)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"scaled=\",sess.run(logits_scaled))\n",
    "    print(\"scaled2=\",sess.run(logits_scaled2))\n",
    "    # 经过第二次的softmax后，分布概率会有变化\n",
    "    \n",
    "    print(\"rel1=\",sess.run(result1))\n",
    "    print(\"rel2=\",sess.run(result2))\n",
    "    #如果将softmax变换完的值放进去，就相当于第二次softmax的loss,所以会出错\n",
    "    print(\"rel3=\",sess.run(result3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logits里面的值原本加和都是大于1的。但是经过softmax之后，总和变成了1。样本中第一个是跟标签分类相符的，第二与标签分类不符，所以第一个的交叉熵比较小，是0.02215516，而第二个比较大，是3.09967351。\n",
    "\n",
    "> 比较scaled和scaled2可以看到：经过第二次softmax后，分布概率会有变化，而scaled才是我们真是转化的softmax值。比较rel1和rel2可以看到：传入softmax_cross_entropy_with_logits的logits是不需要进行softmax的。如果将softmax后的值scaled传入softmax_cross_entropy_with_logits就相当于进行了两次的softmax转换。\n",
    "\n",
    "对于已经用softmax转换过的scaled，在计算loss时就不能再用TensorFlow里面的softmax_cross_entropy_with_logits了。\n",
    "### one_hot实验\n",
    "\n",
    "对非one_hot编码为标签的数据进行交叉熵计算，比较其与ont-hot编码的交叉熵之间的差别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel4= [2.1721554 2.7696736]\n"
     ]
    }
   ],
   "source": [
    "# 标签总概率为1\n",
    "labels = [[0.4,0.1,0.5],[0.3,0.6,0.1]]\n",
    "\n",
    "result4 = tf.nn.softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                 logits=logits)\n",
    "with tf.Session() as sess:\n",
    "    print(\"rel4=\",sess.run(result4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较前面rel1发现，对于正确分类的交叉熵和错误分类的交叉熵，二者的结果差别没有标准one_hot那么明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算loss值\n",
    "在真正神经网络中，得到之前代码中的一个数组并不能满足要求，还需要对其求均值，是其最终变成一个具体的数值。\n",
    "### 实例描述\n",
    "通过对前面交叉熵结果result1与softmax后的结果logits_scaled计算loss，验证下面结论：\n",
    "1. 对于softmax_cross_entropy_with_logits后的结果求loss直接取均值。\n",
    "2. 对于softmax后的结果使用-tf.reduce_sum(labels*tf.log(logits_scaled))求loss。\n",
    "3. 对于softmax后的结果使用-tf.resuce_sum(labels*tf.log(logits_scaled),1)等同于softmax_cross_entropy_with_logits结果。\n",
    "4. 由（1）和（3）可以推出对（3）进行求均值也可以得出正确的loss值，合并起来的公式为：tf.reduce_sum(-tf.resuce_sum(labels*tf.log(logits_scaled),1) = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.1218286\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_sum(result1)\n",
    "with tf.Session() as sess:\n",
    "    print(\"loss:\",sess.run(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对于rel3这种已经求得softmax的情况求loss，可以把公式进一步简化成：\n",
    "loss2 = -tf.reduce_sum(labels*tf.log(logits_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[0,0,1],[0,1,0]]\n",
    "loss2 = tf.reduce_sum(labels*tf.log(logits_scaled))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
