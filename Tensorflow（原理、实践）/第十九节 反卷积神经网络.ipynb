{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反卷积网络，通过测量输出和已知输入重构未知输入的过程。在神经网络中，反卷积过程并不具备学习的能力，仅仅是**用于可视化**一个已经训练好的卷积网络模型，没有学习训练的过程。\n",
    "\n",
    "下图为VGG 16反卷积神经网络结构，展示了一个卷积网络与反卷积网络结合的过程。VGG 16是一个深度神经网络模型。它的反卷积就是将中间的数据，按照前面卷积、池化等变化的过程，完全相反地做一遍，从而得到类似原始输入的数据。\n",
    "![](imgs/19_vgg16.png)\n",
    "\n",
    "## 反卷积神经网络应用场景\n",
    "由于反卷积网络的特性，导致它有许多特别的应用，一般可以用于**信道均衡、图像恢复、语音识别、地震学、无损探伤**等未知输入估计和过程辨别方面的问题。\n",
    "\n",
    "在神经网络的研究中，反卷积更多是冲淡可视化的作用。对于一个复杂的深度卷积网络，通过每层若干个卷积核的变换，我们无法知道每个卷积核关注的是什么，变换后的特征是什么样子。通过**反卷积的还原**，可以对这些问题有个清晰的可视化，以各层得到的特征图作为输入，进行反卷积，得到反卷积结果，用以验证显示各层提取到的特征图。\n",
    "\n",
    "## 反卷积原理\n",
    "反卷积，可以理解为卷积操作的逆操作。这里千万不要当成反卷积操作可以复原卷积操作的输入值，反卷积并没有这个功能，它仅仅是将卷积变换过程中的步骤反向变换一个而已，通过将卷积核转置，与卷积后的结果再做一遍卷积，所以它还有个名字叫**转置卷积**。\n",
    "虽然它不能还原出原来卷积的样子，但是在作用上具有类似的效果，可以将带有小部分缺失的信息最大化地恢复，也可以用来恢复被卷积生成后的原始输入。\n",
    "\n",
    "反卷积具体操作比较复杂，具体步骤如下：\n",
    "1. 首先是将卷积核反转（并不是转置，而是上下左右方向进行递序操作）。\n",
    "2. 再将卷积结果作为输入，做补0的扩充操作，即往每一个元素后面补0.这一步是根据步长来的，对每一个元素沿着步长的方向补（步长-1）个0.例如，步长为1就不用补0了。\n",
    "3. 在扩充后的输入基础上再对整体补0.以原始输入的shape作为输出，按照前面卷积padding规则，计算padding的补0位置及个数，得到的补0位置要上下和左右各自颠倒一下。\n",
    "4. 将补0后的卷积结果作为真正的输入，翻转后的卷积核为filter，进行步长为1的卷积操作。\n",
    "\n",
    "> 计算padding按规则补0 时，统一按照padding='SAME'、步长为1x1的方式来计算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
