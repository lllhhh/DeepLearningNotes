{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 损失函数--用真实值与预测值的距离来知道模型的收敛方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数介绍\n",
    "损失函数的作用是用于描述模型预测值与真实值的差距大小。一般有两种常见的算法--**均值平方差(MSE)**和**交叉熵** 。\n",
    "## 1. 均值平方差\n",
    "均值平方差（Mean Squared Error,MSE），也称“均方误差”，在神经网络中主要是表达预测值与真实值之间的差异，在数理统计中，均方误差是指参数估计值与参数真值之差平方的期望值。公式为：$$ MSE= \\dfrac{1}{n} \\sum_{t=1}^n (observde_t - predicted_t)^2 $$\n",
    "均方误差的值越小，表明模型越好。类似的损失算法还有均方根误差**RMSE(将MSE开平方)**、**平均绝对值误差MAD（对一个真实值与预测值相减的绝对值取平方值**）。\n",
    "> 在神经网络中，预测值与真实值控制在同样的数据分布内，假设将预测值经过Sigmoid激活函数得到取值范围在0~1之间，那么真实值也归一化成0~1之间。这样在做loss计算时才会有好的效果。\n",
    "\n",
    "## 2.交叉熵\n",
    "交叉熵（crossentropy）也是loss算法的一种，一般用在分类问题上，表达的意思为预测输入样本属于某一类的概率。表达式：\n",
    "$$ c=-\\dfrac{1}{n} \\sum_{x} [y lna + (1-y)ln(1-a)] $$\n",
    "其中y代表真实值分类(0或1)，a代表预测值。交叉熵也是值越小，代表预测结果越准确。\n",
    "> 这里用于计算的a也是通过分布统一化处理的（或者是经过Sigmoid函数激活的），取值范围在0~1之间。如果真实值和预测值都是1，前面一项y*ln(a)就是1*ln(1)等于0，后一项(1-y)*ln(1-a)也就是0*ln(0)等于0，loss为0，反之loss函数为其他函数。\n",
    "## 损失算法的选取\n",
    "损失函数的选取取决于输入标签数据的类型：如果输入的是实数、无界的值，损失函数使用平方差，如果输入标签是位矢量（分类标志），使用交叉熵会更合适。\n",
    "\n",
    "# TensorFlow中常见loss函数\n",
    "## 1.均值平方差\n",
    "在TensorFlow中没有单独的MSE函数，不过由于公式比较简单，往往开发者会自己组合：\n",
    "___\n",
    "```Python\n",
    "MSE = tf.reduce_mean(tf.pow(tf.sub(logits,outputts)))\n",
    "MSE = tf.reduce_mean(tf.sqar(tf.sub(logits,outputs)))\n",
    "MSE = tf.reduce_mean(tf.sqar(logits-outputs))\n",
    "```\n",
    "___\n",
    "代码中logits代表标签值，outputs代表预测值。也可以组合成其他函数：\n",
    "___\n",
    "```Python\n",
    "Rmse = tf.sqrt(tf.reduce_mean(tf.pow(tf.sub(logits,outputs),2.0)))\n",
    "mad = tf.reduce_mean(tf.complex_abs(tf.sub(logits,outputs)))\n",
    "```\n",
    "___\n",
    "## 2.交叉熵\n",
    "在TensorFlow中常见的交叉熵函数有:\n",
    "- Sigmoid交叉熵\n",
    "- softmax交叉熵\n",
    "- Sparse交叉熵\n",
    "- 加权Signoid交叉熵。\n",
    "在TensorFlow中常用的损失函数表：\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>操作</th>\n",
    "        <th>描述</th>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <th>tf.nn.sigmoid_cross_entropy_with_logits(logits,targets,name=None)</th>\n",
    "        <th>计算输入logits和targets的交叉熵</th>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <th>tf.nn.softmax_cross_entropy_with_logits(logits,labels,name=None)</th>\n",
    "        <th>计算logits和labels的softmax交叉熵 Logits和Label必须为相同的shape与数据类型</th>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <th>tf.nn.sparse_softmax_cross_entropy_with_logits(logits,labels,name=None)</th>\n",
    "        <th>计算logits和labels的softmax交叉熵,与softmax_cross_entropy_with_logits功能一样，区别在于sparse_softmax_cross_entropy_with_logits的样本真实值与预测结果不需要one_hot编码，但是要求分类的个数一定要从0开始。假如分2类，那么标签的预测值只有0和1两个数。如果是5类，就是01234这5个数</th>\n",
    "    </tr>\n",
    "    \n",
    "    <tr>\n",
    "        <th>tf.nn.weighted_cross_entropy_with_logits(logits,targets,pos_weight,name=None)</th>\n",
    "        <th>在交叉熵的基础上给第一项乘以一个系数**（加权）**，是增加或减少正样本在计算交叉熵时的损失值</th>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "# softmax算法与损失函数的综合应用\n",
    "## 交叉熵实验\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
