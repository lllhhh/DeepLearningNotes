{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 损失函数--用真实值与预测值的距离来知道模型的收敛方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数介绍\n",
    "损失函数的作用是用于描述模型预测值与真实值的差距大小。一般有两种常见的算法--**均值平方差(MSE)**和**交叉熵** 。\n",
    "## 1. 均值平方差\n",
    "均值平方差（Mean Squared Error,MSE），也称“均方误差”，在神经网络中主要是表达预测值与真实值之间的差异，在数理统计中，均方误差是指参数估计值与参数真值之差平方的期望值。公式为：$$ MSE= \\dfrac{1}{n} \\sum_{t=1}^n (observde_t - predicted_t)^2 $$\n",
    "均方误差的值越小，表明模型越好。类似的损失算法还有均方根误差**RMSE(将MSE开平方)**、**平均绝对值误差MAD（对一个真实值与预测值相减的绝对值取平方值**）。\n",
    "> 在神经网络中，预测值与真实值控制在同样的数据分布内，假设将预测值经过Sigmoid激活函数得到取值范围在0~1之间，那么真实值也归一化成0~1之间。这样在做loss计算时才会有好的效果。\n",
    "\n",
    "## 2.交叉熵\n",
    "交叉熵（crossentropy）也是loss算法的一种，一般用在分类问题上，表达的意思为预测输入样本属于某一类的概率。表达式：\n",
    "$$ c=-\\dfrac{1}{n} \\sum_{x} [y lna + (1-y)ln(1-a)] $$\n",
    "其中y代表真实值分类(0或1)，a代表预测值。交叉熵也是值越小，代表预测结果越准确。\n",
    "> 这里用于计算的a也是通过分布统一化处理的（或者是经过Sigmoid函数激活的），取值范围在0~1之间。如果真实值和预测值都是1，前面一项y*ln(a)就是1*ln(1)等于0，后一项(1-y)*ln(1-a)也就是0*ln(0)等于0，loss为0，反之loss函数为其他函数。\n",
    "## 损失算法的选取\n",
    "损失函数的选取取决于输入标签数据的类型：如果输入的是shi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
