{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单循环神经网络实现--裸写一个退位减法器\n",
    "本例是一个纯手写的代码例子，使用Python手动搭建一个简单的RNN网络，让它来拟合一个退位减法。退位减法也具有RNN的特性，即输入的两个数相减时，一旦发生退位运算，需要将中间状态保存起来，当高位的数传入时将退位标志一并传入参与运算。\n",
    "\n",
    "## 1.定义基本函数\n",
    "首先手动写一个Sigmoid函数及其导数（导数用于反向传播）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0) #随机数生成器的种子，可以每次得到一样的值\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x): #激活函数\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):#激活函数的导数\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.建立二进制映射\n",
    "定义的减法最大限制在256之内，即8位二进制的减法，定义int与二进制之间的映射数组int2binary。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2binary = {} #整数到其二进制表示的映射\n",
    "binary_dim = 8 #暂时制作256以内的减法\n",
    "## 计算0-256的二进制表示\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.定义参数\n",
    "定义学习参数：印象层的权重synapse_0、循环节点的权重synapse_h（输入节点16、输出节点16）、输出层的权重sunapse_1（输入16节点，输出1节点）。为了减小复杂度，这里只设置w权重，b被忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables\n",
    "alpha = 0.9 #学习速率\n",
    "input_dim = 2 #输入的维度是2\n",
    "hidden_dim = 16 \n",
    "output_dim = 1 #输出维度为1\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = (2*np.random.random((input_dim,hidden_dim)) - 1)*0.05 #维度为2*16， 2是输入维度，16是隐藏层维度\n",
    "synapse_1 = (2*np.random.random((hidden_dim,output_dim)) - 1)*0.05\n",
    "synapse_h = (2*np.random.random((hidden_dim,hidden_dim)) - 1)*0.05\n",
    "# => [-0.05, 0.05)，\n",
    "\n",
    "# 用于存放反向传播的权重更新值\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "synapse_0_update在前面很少见到，是因为它被隐含在优化器里了。这里全部“裸写”（不使用Tensor Flow库函数），需要定义一组变量，用于反向优化参数时存放参数需要调整的调整值，对应于前面的3个权重synapse_0、synapse_1和synapse_h。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.准备样本数据\n",
    "1. 建立循环生成样本数据，先生成两个数a和b。如果a小于b，就交换位置，保证被减数大。\n",
    "2. 计算出相减的结果c。\n",
    "3. 将3个数转换成二进制，为模型计算做准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "for j in range(10000):\n",
    "    \n",
    "    # 生成一个数字a\n",
    "    a_int = np.random.randint(largest_num)\n",
    "    #生成一个数字b,b的最大值取的是largest_number/2,作为被减数，让它小一点\n",
    "    b_int = np.random.randint(largest_num / 2)\n",
    "    #如果生成的b大了，那么交换\n",
    "    if a_int < b_int:\n",
    "        tt = a_int\n",
    "        b_int = a_int\n",
    "        a_int = tt\n",
    "        \n",
    "    a = int2binary[a_int] #二进制编码\n",
    "    b = int2binary[b_int] \n",
    "    #正确答案\n",
    "    c_int = a_int - b_int\n",
    "    c = int2binary[a_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.模型初始化\n",
    "初始化输出值为0，初始化总误差为0，定义layer_2_deltas存储反向传播过程中的循环层的误差，layer_1_values为隐藏层的输出值，由于第一个数据传入时，没有前面的隐藏层输出值来作为本次的输入，所以需要为其定义一个初始值，这里定义为0.1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros_like(c)\n",
    "overallError = 0 #每次把总误差清零\n",
    "layer_2_deltas = list() #存储每个时间点输出层的误差\n",
    "layer_1_values = list() #存储每个时间点隐藏层的值\n",
    "\n",
    "layer_1_values.append(np.ones(hidden_dim)*0.1) #一开始没有隐藏层，所以初始化一下原始值为0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.正向传播\n",
    "循环遍历每个二进制位，从个位开始依次相减，并将中间隐藏层的输出传入下一位的计算（退位减法），把每一个时间点的误差导数都记录下来，同时统计总误差，为输出准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for position in range(binary_dim):#循环遍历每一个二进制位\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])#从右到左，每次去两个输入数字的一个bit位\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T#正确答案\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))#（输入层 + 之前的隐藏层） -> 新的隐藏层，这是体现循环神经网络的最核心的地方！！！\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1)) #隐藏层 * 隐藏层到输出层的转化矩阵synapse_1 -> 输出层\n",
    "        \n",
    "        layer_2_error = y - layer_2 #预测误差\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2)) #把每一个时间点的误差导数都记录下来\n",
    "        overallError += np.abs(layer_2_error[0])#总误差\n",
    "    \n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0]) #记录下每一个预测bit位\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))#记录下隐藏层的值，在下一个时间点用\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一行代码是为了反向传播准备的初始化。同正向传播一样，反向传播是从最后一次往前反向计算误差，对于每一个当前的计算都需要有它的下一次结果参与。\n",
    "反向计算是从最后一次开始的，它没有后一次的输出，所以需要初始化一个值作为其后一次的输入，这里初始化为0。\n",
    "\n",
    "## 7.反向训练\n",
    "初始化之后，开始从高位往回遍历，一次对每一位的所有层计算误差，并根据每层误差对权重求偏导，得到其调整值，最终将每一位算出的各层权重的调整值加在一起乘以学习率，来更新各层的权重，完成一次优化训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #反向传播，从最后一个时间点到第一个时间点\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]]) #最后一次的两个输入\n",
    "        layer_1 = layer_1_values[-position-1] #当前时间点的隐藏层\n",
    "        prev_layer_1 = layer_1_values[-position-2] #前一个时间点的隐藏层\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1] #当前时间点输出层导数\n",
    "        # error at hidden layer\n",
    "        # 通过后一个时间点（因为是反向传播）的隐藏层误差和当前时间点的输出层误差，计算当前时间点的隐藏层误差\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        \n",
    "       # 等到完成了所有反向传播误差计算， 才会更新权重矩阵，先暂时把更新矩阵存起来。\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "    # 完成所有反向传播之后，更新权重矩阵。并把矩阵变量清零\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新完之后将中间变量值清零。\n",
    "## 8.输出结果\n",
    "没运行800次将结果输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(j % 800 == 0):\n",
    "        #print(synapse_0,synapse_h,synapse_1)\n",
    "        print(\"总误差:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int) + \" - \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总误差:[3.97242498]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 0 0 0 0 0 0]\n",
      "9 - 9 = 0\n",
      "------------\n",
      "总误差:[2.1721182]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 0 1 0 0 0 1]\n",
      "17 - 0 = 0\n",
      "------------\n",
      "总误差:[1.1082385]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 0 0 0 0 0 0]\n",
      "59 - 59 = 0\n",
      "------------\n",
      "总误差:[0.18727913]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 0 0 0 0 0 0]\n",
      "19 - 19 = 0\n",
      "------------\n",
      "总误差:[0.21914293]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 0 0 0 0 0 0]\n",
      "71 - 71 = 0\n",
      "------------\n",
      "总误差:[0.26861004]\n",
      "Pred:[0 0 1 1 1 1 0 0]\n",
      "True:[0 0 1 1 1 1 0 0]\n",
      "71 - 11 = 60\n",
      "------------\n",
      "总误差:[0.11815367]\n",
      "Pred:[1 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 0 0]\n",
      "230 - 102 = 128\n",
      "------------\n",
      "总误差:[0.2927243]\n",
      "Pred:[0 1 1 1 0 0 0 1]\n",
      "True:[0 1 1 1 0 0 0 1]\n",
      "160 - 47 = 113\n",
      "------------\n",
      "总误差:[0.04298749]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 0 0 0 0 0 0]\n",
      "3 - 3 = 0\n",
      "------------\n",
      "总误差:[0.04243453]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 0 0 0 0 0 0]\n",
      "17 - 17 = 0\n",
      "------------\n",
      "总误差:[0.04588656]\n",
      "Pred:[1 0 0 1 0 1 1 0]\n",
      "True:[1 0 0 1 0 1 1 0]\n",
      "167 - 17 = 150\n",
      "------------\n",
      "总误差:[0.08098026]\n",
      "Pred:[1 0 0 1 1 0 0 0]\n",
      "True:[1 0 0 1 1 0 0 0]\n",
      "204 - 52 = 152\n",
      "------------\n",
      "总误差:[0.03262333]\n",
      "Pred:[1 1 0 0 0 0 0 0]\n",
      "True:[1 1 0 0 0 0 0 0]\n",
      "209 - 17 = 192\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "for j in range(10000):\n",
    "    \n",
    "    #生成一个数字a\n",
    "    a_int = np.random.randint(largest_number) \n",
    "    #生成一个数字b,b的最大值取的是largest_number/2,作为被减数，让它小一点。\n",
    "    b_int = np.random.randint(largest_number/2) \n",
    "    #如果生成的b大了，那么交换一下\n",
    "    if a_int<b_int:\n",
    "        tt = a_int\n",
    "        b_int = a_int\n",
    "        a_int=tt\n",
    "    \n",
    "    a = int2binary[a_int] # binary encoding\n",
    "    b = int2binary[b_int] # binary encoding    \n",
    "    # true answer\n",
    "    c_int = a_int - b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # 存储神经网络的预测值\n",
    "    d = np.zeros_like(c)\n",
    "    overallError = 0 #每次把总误差清零\n",
    "    \n",
    "    layer_2_deltas = list() #存储每个时间点输出层的误差\n",
    "    layer_1_values = list() #存储每个时间点隐藏层的值\n",
    "    \n",
    "    layer_1_values.append(np.ones(hidden_dim)*0.1) # 一开始没有隐藏层，所以初始化一下原始值为0.1\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):#循环遍历每一个二进制位\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])#从右到左，每次去两个输入数字的一个bit位\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T#正确答案\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))#（输入层 + 之前的隐藏层） -> 新的隐藏层，这是体现循环神经网络的最核心的地方！！！\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1)) #隐藏层 * 隐藏层到输出层的转化矩阵synapse_1 -> 输出层\n",
    "        \n",
    "        layer_2_error = y - layer_2 #预测误差\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2)) #把每一个时间点的误差导数都记录下来\n",
    "        overallError += np.abs(layer_2_error[0])#总误差\n",
    "    \n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0]) #记录下每一个预测bit位\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))#记录下隐藏层的值，在下一个时间点用\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    #反向传播，从最后一个时间点到第一个时间点\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]]) #最后一次的两个输入\n",
    "        layer_1 = layer_1_values[-position-1] #当前时间点的隐藏层\n",
    "        prev_layer_1 = layer_1_values[-position-2] #前一个时间点的隐藏层\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1] #当前时间点输出层导数\n",
    "        # error at hidden layer\n",
    "        # 通过后一个时间点（因为是反向传播）的隐藏层误差和当前时间点的输出层误差，计算当前时间点的隐藏层误差\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        \n",
    "       # 等到完成了所有反向传播误差计算， 才会更新权重矩阵，先暂时把更新矩阵存起来。\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "    # 完成所有反向传播之后，更新权重矩阵。并把矩阵变量清零\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "   \n",
    "    # print out progress\n",
    "    if(j % 800 == 0):\n",
    "        #print(synapse_0,synapse_h,synapse_1)\n",
    "        print(\"总误差:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int) + \" - \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，刚开始还不准，随着迭代次数增加，到后来已经可以完全拟合退位减法了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
