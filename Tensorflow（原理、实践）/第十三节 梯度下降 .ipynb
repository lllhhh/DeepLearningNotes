{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降--让模型逼近最小偏差\n",
    "# 梯度下降的作用及分类\n",
    "梯度下降法是一个最优算法，常用于机器学习和人工智能中递归性地逼近最小偏差模型，梯度下降的方向也就是用负梯度方向为搜索方向，沿着梯度下降的方向求解极小值。\n",
    "\n",
    "在训练过程中，每次的正向传播后都会得到输出值与真实值的损失值，这个损失值越小，代表模型越好，于是梯度下降的算法就用在这里，帮助寻找最小的那个损失值，从而可以反推出对应的学习参数b和w，达到优化模型的效果。\n",
    "\n",
    "> 常用的梯度下降方法可以分为：**批量梯度下降**、**随机梯度下降**和**小批量梯度下降**。\n",
    "\n",
    "- 批量梯度下降：遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度和更新梯度。这种方法每更新一次参数，都要把数据集里的所有样本看一遍，计算量大，计算速度慢，不支持在线学习，成为Batch gradient descent，批量梯度下降。\n",
    "- 随机梯度下降：每看一个数据就算一下损失函数，然后求梯度更新参数，这称为stochastic gradient descent，随机梯度下降。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，命中不到最优点。两次参数的更新也有可能互相抵消，造成目标函数震荡比较剧烈。\n",
    "- 小批量梯度下降：为了克服上面两种方法的缺点，一般采用一种折中手段--小批量梯度下降。这种方法把数据分为若干个批次，按批来更新参数，这样一批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。\n",
    "\n",
    "# TensorFlow中的梯度下降函数\n",
    "在TensorFlow中通过一个叫Optimizer的优化器类进行训练优化的。对于不同算法的优化器，在TensorFlow中会有不同的类。\n",
    "![梯度下降优化器](imgs/13_gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练过程中，先实例一个优化函数如tf.train.GradientDescentOptimizer,并基于一定的学习率进行梯度优化训练：\n",
    "___\n",
    "```Python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "```\n",
    "___\n",
    "接着使用一个minimize()操作，里面传入损失值节点loss，再启动一个外层的循环，优化器就会按照循环的次数一次次沿着los"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
