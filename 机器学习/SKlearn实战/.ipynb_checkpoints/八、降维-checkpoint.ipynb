{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 降维肯定会丢失一些信息（好比将图片压缩成JPEG格式会降低图像质量）。\n",
    "- 这种方法可以加快训练速度，同时会让系统表现的稍微差一点。\n",
    "- 降维会让工作流水线更复杂因尔更难维护。\n",
    "- 应该先尝试使用原来的数据来训练，如果训练速度太慢再考虑使用降维。\n",
    "- 通常不会发生，降低训练集数据的维度可能会筛选掉一些噪音和不必要的细节，让结果比降维前更好的情况。\n",
    "- 除了加快训练速度外，在可视化方面也十分有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降维方法\n",
    "投影（projection）和流行学习（Manifold Learing）。\n",
    "## 流行的降维技术\n",
    "- 主成分分析（PCA）\n",
    "- 核主成分分析（Kernel PCA）\n",
    "- 局部线性嵌入（LLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析\n",
    "- 找到接近数据集分布的超平面。\n",
    "- 将所有的数据都投影到这个超平面上。\n",
    "### 保留（最大）方差\n",
    "- 选择保持最大方差的轴看起来是合理的，因为它很可能比其他投影损失更少的信息。\n",
    "- 证明这种选择的另一种方法是，选择这个轴使得将原始数据集投影到该轴上的均方距离最小。\n",
    "### 主成分(Principle Components)\n",
    "- 主成分的方向不稳定：稍微打乱一下训练集并再次运行PCA，则某些新PC可能会指向与原始PC方向相反。\n",
    "- 它们通常是在同一轴线上。\n",
    "- 某些情况下，一对PC甚至可能会旋转或交换，但它们定义的平面通常保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用Numpy的svd()方法获得训练集的所有主成分，然后提取前两个PC：\n",
    "X_centered = X-X.mean(axis=0)\n",
    "U,s,V=np.linalg.svd(X_centered)\n",
    "c1=V.T[:,0]\n",
    "c2=V.T[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##使用sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方差解释率（Explained Variance Ratio）\n",
    "表示每个主成分轴上的数据集方差的比例。\n",
    "```python\n",
    "print(pca.explained_variance_ratio_)\n",
    "array([0.84248607, 0.14631839])\n",
    "```\n",
    "表明，84.2%数据集方差位于第一轴，14.6%方差位于第二轴。第三轴比例不到1.2%。因此可认为它没有包含什么信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择正确的维度\n",
    "## PCA压缩\n",
    "## 增量PCA（IPCA）\n",
    "将训练集分批，并一次只对一个批量使用IPCA算法。这对大型训练集非常有用，并且可以在线应用PCA。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "import numpy as np\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_mnist_reduced = inc_pca.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机PCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
